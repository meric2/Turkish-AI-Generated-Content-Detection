https://arxiv.org/abs/2401.16587

A widely used tool for language analysis is the Linguistic Inquiry and Word Count (LIWC). This tool includes dictionaries that categorize words into various classes such as pronouns, prepositions, conjunctions, articles, and words conveying positive and negative emotions, or relating to friends and family.

Tausczik, Y. R., and Pennebaker, J. W. The psychological meaning of words: LIWC and computerized text analysis methods. Journal of Language and Social Psychology 29, 1 (2010), 24–54.

Dataset:
Rashkin, H., Smith, E. M., Li, M., and Boureau, Y.-L. Towards empathetic open-domain conversation models: A new benchmark and dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Florence, Italy, July 2019), Association for Computational Linguistics, pp. 5370–5381. (human)
(Explained how the ai generated part is created)

--------------------

https://arxiv.org/abs/2109.13296

To detect AI-generated texts, TuringBench utilizes a variety of state-of-the-art machine learning and deep learning models. Specifically, they employ models like RoBERTa, BERT, and other neural network architectures such as N-gram based models, SVMs, and specialized AI detectors like the OpenAI detector. These models are trained and evaluated on tasks such as Authorship Attribution and Turing Test scenarios.
https://turingbench.ist.psu.edu/
https://github.com/AdaUchendu/TuringBench

--------------------

https://arxiv.org/abs/2309.08913

Our primary contribution is a statistical framework for human detection that can be used to quantify the detectability of a machine for a given human detection context that is agnostic to content modality and generation task. The framework provides the language required to analyze important aspects of the human detection problem such as the progression of a family of models towards human-like abilities and the effectiveness of different classifiers for human detection. Secondary to the proposed framework is the contribution of a human detection method, ProxiHuman, that utilizes the geometry of a machine’s embedding space to determine the origin (human or machine) of content.

--------------------

https://arxiv.org/abs/2106.11394

The primary goal of the XAI framework is to identify which types of explanations and models provide the highest levels of transparency and trustworthiness from the user's perspective. By standardizing the evaluation of modal interpretability, the framework aims to guide the development of more transparent and user-friendly AI systems.

--------------------

https://arxiv.org/abs/2403.19725

Uses classical methods. Word2Vec/GloVe: Pre-trained word embedding models used to capture semantic meaning in the texts.

--------------------

https://arxiv.org/abs/2310.05130

FAST-DETECTGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature
Faster and better than DetectGPT

--------------------

https://arxiv.org/abs/2403.03506

Roberta: The authors employed the Roberta model, a robustly optimized BERT pretraining approach, to leverage its strong language understanding capabilities for detecting AI-generated text within hybrid texts.

For text segmentation: This method treats text segmentation as a supervised learning task, helping in distinguishing between segments of text generated by AI and those by humans​​. They used a "Transformer over pre-trained transformer" approach to improve neural text segmentation by enhancing topic coherence​​. Cross segment attention was applied to aid in the segmentation and detection tasks​.

Dataset: The Coauthor dataset was utilized, which is designed specifically for exploring language model capabilities in human-AI collaborative writing scenarios​.

Additional Methods: 
DetectGPT: This zero-shot detection method uses probability curvature to identify machine-generated text​​.
SynSciPass: Used for detecting appropriate uses of scientific text generation, contributing to the understanding of AI-generated scientific text.

--------------------

https://arxiv.org/abs/2403.01152

The history of AI detectors and how they work.

--------------------

https://arxiv.org/abs/2402.11167

Token-Ensemble Generation: This technique generates text by selecting the next token from a random candidate pool of tokens, which are generated from multiple instances of large language models (LLMs). This process creates a diverse set of outputs, making it harder for detection models to identify patterns typical of AI-generated text.

Evaluation Metrics: AUROC (Area Under the Receiver Operating Characteristic Curve): Used to evaluate the performance of detection models. An AUROC score of 0.5 indicates random detection capability, while a score of 1.0 indicates perfect detection capability.

Datasets:
XSum: A dataset used for summarization tasks.
SQuAD: A dataset for question answering.
Writing Prompts: Used to evaluate performance on creative writing tasks.

Detection Models:
The study tests the effectiveness of the token-ensemble attacks against various detection models, including GPT-2, OPT, Neo, and GPT-J, by comparing their performance with and without the token-ensemble method.

Results: The results indicate that the token-ensemble generation method significantly reduces the detection accuracy of various AI-generated text detectors across different datasets. This highlights the vulnerability of current detection models to such attacks.

--------------------

https://arxiv.org/abs/2401.12970

Invariance-Based Detection: This method involves generating multiple paraphrases of the input text and checking the consistency of the language model's output. The invariance is measured using metrics like the bag-of-words edit distance and the Levenshtein score.

Equivariance-Based Detection: This technique uses transformations and inverse transformations of the input text. It compares the outputs generated from these transformations to detect discrepancies that suggest machine-generated content.

Uncertainty-Based Detection: This method relies on the model's uncertainty in generating text. It evaluates the consistency of the outputs produced by the model when given slightly altered input prompts.

These algorithms aim to exploit the unique characteristics of machine-generated text, such as higher consistency and specific error patterns, to differentiate it from human-written content. The study uses various datasets, including news articles, creative writing, student essays, code snippets, Yelp reviews, and arXiv abstracts, to validate the effectiveness of these detection methods.

--------------------