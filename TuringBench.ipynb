{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TuringBench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models**\n",
    "- Bert\n",
    "- RoBERTa\n",
    "- GPT-2\n",
    "- XLNet\n",
    "- Grover\n",
    "- XLM\n",
    "- CTRL\n",
    "\n",
    "This notebook does AI-generated Text Detection with different NLP models. In a proposed method, adversarial training is applied through these models. The aim is to enhance the original models' performances by utilizing this method through the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AdamW, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import XLMTokenizer, XLMForSequenceClassification\n",
    "from transformers import CTRLTokenizer, CTRLForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('reviews.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for training\n",
    "texts = data['review'].tolist()\n",
    "labels = data['label'].apply(lambda x: 1 if x == 'ai' else 0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Prepare the data for BERT\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, train_labels)\n",
    "test_dataset = TextDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',     # Evaluation strategy to adopt during training\n",
    "    eval_steps=100,                  # Evaluation step to perform evaluation\n",
    "    save_steps=100,                  # Save checkpoint every X steps\n",
    "    log_level='info',                # Set logging level to info\n",
    "    log_level_replica='info'         # Save checkpoint every X steps\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "bert_results = trainer.evaluate()\n",
    "print(bert_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform prediction on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions\n",
    "pred_labels = np.argmax(preds, axis=1)  # Get the predicted labels\n",
    "\n",
    "# If you need the true labels\n",
    "true_labels = [example['labels'] for example in test_dataset]\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(true_labels, pred_labels, target_names=['Human', 'AI'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './test/bert_model'\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "\n",
    "# Tokenization\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, train_labels)\n",
    "test_dataset = TextDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_roberta',  # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs_roberta',    # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',     # Evaluation strategy to adopt during training\n",
    "    eval_steps=100,                  # Evaluation step to perform evaluation\n",
    "    save_steps=100,                  # Save checkpoint every X steps\n",
    "    log_level='info',                # Set logging level to info\n",
    "    log_level_replica='info'         # Save checkpoint every X steps\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "roberta_results = trainer.evaluate()\n",
    "print(roberta_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform prediction on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions\n",
    "pred_labels = np.argmax(preds, axis=1)  # Get the predicted labels\n",
    "\n",
    "# If you need the true labels\n",
    "true_labels = [example['labels'] for example in test_dataset]\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(true_labels, pred_labels, target_names=['Human', 'AI'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './test/roberta_model'\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
    "\n",
    "# Prepare the data for GPT-2\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, train_labels)\n",
    "test_dataset = TextDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_gpt2',     # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',     # Evaluation strategy to adopt during training\n",
    "    eval_steps=100,                  # Evaluation step to perform evaluation\n",
    "    save_steps=100,                  # Save checkpoint every X steps\n",
    "    log_level='info',                # Set logging level to info\n",
    "    log_level_replica='info'         # Save checkpoint every X steps\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "gpt2_results = trainer.evaluate()\n",
    "print(gpt2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform prediction on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions\n",
    "pred_labels = np.argmax(preds, axis=1)  # Get the predicted labels\n",
    "\n",
    "# If you need the true labels\n",
    "true_labels = [example['labels'] for example in test_dataset]\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(true_labels, pred_labels, target_names=['Human', 'AI'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './test/gpt2_model'\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n",
    "\n",
    "# Prepare the data for XLNet\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, train_labels)\n",
    "test_dataset = TextDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_xlnet',    # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs_xlnet',      # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',     # Evaluation strategy to adopt during training\n",
    "    eval_steps=100,                  # Evaluation step to perform evaluation\n",
    "    save_steps=100,                  # Save checkpoint every X steps\n",
    "    log_level='info',                # Set logging level to info\n",
    "    log_level_replica='info'         # Save checkpoint every X steps\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform prediction on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions\n",
    "pred_labels = np.argmax(preds, axis=1)  # Get the predicted labels\n",
    "\n",
    "# If you need the true labels\n",
    "true_labels = [example['labels'] for example in test_dataset]\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(true_labels, pred_labels, target_names=['Human', 'AI'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './test/xlnet_model'\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model from the Grover model\n",
    "tokenizer = AutoTokenizer.from_pretrained('grover-base')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('grover-base', num_labels=2)\n",
    "\n",
    "# Prepare the data for Grover\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, train_labels)\n",
    "test_dataset = TextDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_grover',    # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs_grover',      # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',     # Evaluation strategy to adopt during training\n",
    "    eval_steps=100,                  # Evaluation step to perform evaluation\n",
    "    save_steps=100,                  # Save checkpoint every X steps\n",
    "    log_level='info',                # Set logging level to info\n",
    "    log_level_replica='info'         # Save checkpoint every X steps\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "grover_results = trainer.evaluate()\n",
    "print(grover_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform prediction on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions\n",
    "pred_labels = np.argmax(preds, axis=1)  # Get the predicted labels\n",
    "\n",
    "# If you need the true labels\n",
    "true_labels = [example['labels'] for example in test_dataset]\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(true_labels, pred_labels, target_names=['Human', 'AI'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './test/grover_model'\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model\n",
    "tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\n",
    "model = XLMForSequenceClassification.from_pretrained('xlm-mlm-en-2048', num_labels=2)\n",
    "\n",
    "# Prepare the data for XLM\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, train_labels)\n",
    "test_dataset = TextDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_xlm',    # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs_xlm',      # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',     # Evaluation strategy to adopt during training\n",
    "    eval_steps=100,                  # Evaluation step to perform evaluation\n",
    "    save_steps=100,                  # Save checkpoint every X steps\n",
    "    log_level='info',                # Set logging level to info\n",
    "    log_level_replica='info'         # Save checkpoint every X steps\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "xlm_results = trainer.evaluate()\n",
    "print(xlm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform prediction on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions\n",
    "pred_labels = np.argmax(preds, axis=1)  # Get the predicted labels\n",
    "\n",
    "# If you need the true labels\n",
    "true_labels = [example['labels'] for example in test_dataset]\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(true_labels, pred_labels, target_names=['Human', 'AI'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './test/xlm_model'\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model\n",
    "tokenizer = CTRLTokenizer.from_pretrained('ctrl')\n",
    "model = CTRLForSequenceClassification.from_pretrained('ctrl', num_labels=2)\n",
    "\n",
    "# Prepare the data for CTRL\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, train_labels)\n",
    "test_dataset = TextDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_ctrl',    # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs_ctrl',      # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',     # Evaluation strategy to adopt during training\n",
    "    eval_steps=100,                  # Evaluation step to perform evaluation\n",
    "    save_steps=100,                  # Save checkpoint every X steps\n",
    "    log_level='info',                # Set logging level to info\n",
    "    log_level_replica='info'         # Save checkpoint every X steps\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "ctrl_results = trainer.evaluate()\n",
    "print(ctrl_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform prediction on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions\n",
    "pred_labels = np.argmax(preds, axis=1)  # Get the predicted labels\n",
    "\n",
    "# If you need the true labels\n",
    "true_labels = [example['labels'] for example in test_dataset]\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(true_labels, pred_labels, target_names=['Human', 'AI'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './test/ctrl_model'\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models with Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training(model, inputs, labels, epsilon=0.01):\n",
    "    model.zero_grad()\n",
    "    outputs = model(**inputs)\n",
    "    loss = torch.nn.functional.cross_entropy(outputs.logits, labels)\n",
    "    loss.backward()  # Compute gradients\n",
    "    \n",
    "    # Create perturbed inputs with adversarial noise\n",
    "    perturbed_inputs = inputs.copy()\n",
    "    for key in perturbed_inputs:\n",
    "        if perturbed_inputs[key].requires_grad:\n",
    "            perturbation = epsilon * perturbed_inputs[key].grad.sign()\n",
    "            perturbed_inputs[key] = perturbed_inputs[key] + perturbation.detach()  # detach to avoid further graph tracking\n",
    "\n",
    "    # Clear past gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Re-run the model on perturbed data\n",
    "    perturbed_outputs = model(**perturbed_inputs)\n",
    "    perturbed_loss = torch.nn.functional.cross_entropy(perturbed_outputs.logits, labels)\n",
    "\n",
    "    return loss.item(), perturbed_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, filepath):\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = self.data['review'].tolist()\n",
    "        # Convert labels to integers\n",
    "        self.labels = torch.tensor(self.data['label'].apply(lambda x: 0 if x == 'human' else 1).tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "        input_ids = inputs['input_ids'].squeeze(0)  # Remove batch dimension\n",
    "        return input_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    true_labels, predictions = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_labels in dataloader:\n",
    "            inputs = {'input_ids': batch_inputs}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_labels = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(predicted_labels.numpy())\n",
    "            true_labels.extend(batch_labels.numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    return accuracy, precision, recall, f1, conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load data\n",
    "dataset = TextDataset(tokenizer, 'reviews.csv')\n",
    "\n",
    "# Split the dataset\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set model to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(2):  # Loop over the dataset multiple times\n",
    "    total_loss = 0\n",
    "    total_adv_loss = 0\n",
    "    for batch_inputs, batch_labels in train_loader:\n",
    "        inputs = {'input_ids': batch_inputs, 'labels': batch_labels}\n",
    "        loss, adv_loss = adversarial_training(model, inputs, batch_labels, epsilon=0.01)\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        total_adv_loss += adv_loss\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_adv_loss = total_adv_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss}, Adversarial Loss: {avg_adv_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "\n",
    "# Display confusion matrix for the test set\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './test/bert_adv_model'\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f'Model saved to {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Load data\n",
    "dataset = TextDataset(tokenizer, 'reviews.csv')\n",
    "\n",
    "# Split the dataset\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set model to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(2):  # Loop over the dataset multiple times\n",
    "    total_loss = 0\n",
    "    total_adv_loss = 0\n",
    "    for batch_inputs, batch_labels in train_loader:\n",
    "        inputs = {'input_ids': batch_inputs, 'labels': batch_labels}\n",
    "        loss, adv_loss = adversarial_training(model, inputs, batch_labels, epsilon=0.01)\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        total_adv_loss += adv_loss\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_adv_loss = total_adv_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss}, Adversarial Loss: {avg_adv_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "\n",
    "# Display confusion matrix for the test set\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './test/roberta_adv_model'\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f'Model saved to {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Load data\n",
    "dataset = TextDataset(tokenizer, 'reviews.csv')\n",
    "\n",
    "# Split the dataset\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set model to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(2):  # Loop over the dataset multiple times\n",
    "    total_loss = 0\n",
    "    total_adv_loss = 0\n",
    "    for batch_inputs, batch_labels in train_loader:\n",
    "        inputs = {'input_ids': batch_inputs, 'labels': batch_labels}\n",
    "        loss, adv_loss = adversarial_training(model, inputs, batch_labels, epsilon=0.01)\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        total_adv_loss += adv_loss\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_adv_loss = total_adv_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss}, Adversarial Loss: {avg_adv_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "\n",
    "# Display confusion matrix for the test set\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './test/gpt2_adv_model'\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f'Model saved to {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Load data\n",
    "dataset = TextDataset(tokenizer, 'reviews.csv')\n",
    "\n",
    "# Split the dataset\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set model to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(2):  # Loop over the dataset multiple times\n",
    "    total_loss = 0\n",
    "    total_adv_loss = 0\n",
    "    for batch_inputs, batch_labels in train_loader:\n",
    "        inputs = {'input_ids': batch_inputs, 'labels': batch_labels}\n",
    "        loss, adv_loss = adversarial_training(model, inputs, batch_labels, epsilon=0.01)\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        total_adv_loss += adv_loss\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_adv_loss = total_adv_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss}, Adversarial Loss: {avg_adv_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "\n",
    "# Display confusion matrix for the test set\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './test/xlnet_adv_model'\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f'Model saved to {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('grover-base')\n",
    "\n",
    "# Load data\n",
    "dataset = TextDataset(tokenizer, 'reviews.csv')\n",
    "\n",
    "# Split the dataset\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained('grover-base', num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set model to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(2):  # Loop over the dataset multiple times\n",
    "    total_loss = 0\n",
    "    total_adv_loss = 0\n",
    "    for batch_inputs, batch_labels in train_loader:\n",
    "        inputs = {'input_ids': batch_inputs, 'labels': batch_labels}\n",
    "        loss, adv_loss = adversarial_training(model, inputs, batch_labels, epsilon=0.01)\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        total_adv_loss += adv_loss\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_adv_loss = total_adv_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss}, Adversarial Loss: {avg_adv_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "\n",
    "# Display confusion matrix for the test set\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './test/grover_adv_model'\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f'Model saved to {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\n",
    "\n",
    "# Load data\n",
    "dataset = TextDataset(tokenizer, 'reviews.csv')\n",
    "\n",
    "# Split the dataset\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "model = XLMForSequenceClassification.from_pretrained('xlm-mlm-en-2048', num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set model to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(2):  # Loop over the dataset multiple times\n",
    "    total_loss = 0\n",
    "    total_adv_loss = 0\n",
    "    for batch_inputs, batch_labels in train_loader:\n",
    "        inputs = {'input_ids': batch_inputs, 'labels': batch_labels}\n",
    "        loss, adv_loss = adversarial_training(model, inputs, batch_labels, epsilon=0.01)\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        total_adv_loss += adv_loss\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_adv_loss = total_adv_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss}, Adversarial Loss: {avg_adv_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "\n",
    "# Display confusion matrix for the test set\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './test/xlm_adv_model'\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f'Model saved to {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = CTRLTokenizer.from_pretrained('ctrl')\n",
    "\n",
    "# Load data\n",
    "dataset = TextDataset(tokenizer, 'reviews.csv')\n",
    "\n",
    "# Split the dataset\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "model = CTRLForSequenceClassification.from_pretrained('ctrl', num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set model to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(2):  # Loop over the dataset multiple times\n",
    "    total_loss = 0\n",
    "    total_adv_loss = 0\n",
    "    for batch_inputs, batch_labels in train_loader:\n",
    "        inputs = {'input_ids': batch_inputs, 'labels': batch_labels}\n",
    "        loss, adv_loss = adversarial_training(model, inputs, batch_labels, epsilon=0.01)\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        total_adv_loss += adv_loss\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_adv_loss = total_adv_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss}, Adversarial Loss: {avg_adv_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "\n",
    "# Display confusion matrix for the test set\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './test/ctrl_adv_model'\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f'Model saved to {output_dir}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
