{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AdamW, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import XLMTokenizer, XLMForSequenceClassification\n",
    "from transformers import CTRLTokenizer, CTRLForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('reviews.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for training\n",
    "texts = data['review'].tolist()\n",
    "labels = data['label'].apply(lambda x: 1 if x == 'ai' else 0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = BertForSequenceClassification.from_pretrained(\"./bert_model\")\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(\"./bert_model\")\n",
    "\n",
    "test_encodings = loaded_tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_dataset = TextDataset(test_encodings, test_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',     # Evaluation strategy to adopt during training\n",
    "    eval_steps=100,                  # Evaluation step to perform evaluation\n",
    "    save_steps=100,                  # Save checkpoint every X steps\n",
    "    log_level='info',                # Set logging level to info\n",
    "    log_level_replica='info'         # Save checkpoint every X steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=loaded_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "test_results = trainer.evaluate()\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = RobertaForSequenceClassification.from_pretrained(\"./roberta_model\")\n",
    "loaded_tokenizer = RobertaTokenizer.from_pretrained(\"./roberta_model\")\n",
    "\n",
    "test_encodings = loaded_tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_dataset = TextDataset(test_encodings, test_labels)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',     # Evaluation strategy to adopt during training\n",
    "    eval_steps=100,                  # Evaluation step to perform evaluation\n",
    "    save_steps=100,                  # Save checkpoint every X steps\n",
    "    log_level='info',                # Set logging level to info\n",
    "    log_level_replica='info'         # Save checkpoint every X steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=loaded_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "test_results = trainer.evaluate()\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = GPT2ForSequenceClassification.from_pretrained(\"./gpt2_model\")\n",
    "loaded_tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2_model\")\n",
    "\n",
    "test_encodings = loaded_tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_dataset = TextDataset(test_encodings, test_labels)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',     # Evaluation strategy to adopt during training\n",
    "    eval_steps=100,                  # Evaluation step to perform evaluation\n",
    "    save_steps=100,                  # Save checkpoint every X steps\n",
    "    log_level='info',                # Set logging level to info\n",
    "    log_level_replica='info'         # Save checkpoint every X steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=loaded_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "test_results = trainer.evaluate()\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = XLNetForSequenceClassification.from_pretrained(\"./xlnet_model\")\n",
    "loaded_tokenizer = XLNetTokenizer.from_pretrained(\"./xlnet_model\")\n",
    "\n",
    "test_encodings = loaded_tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_dataset = TextDataset(test_encodings, test_labels)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',     # Evaluation strategy to adopt during training\n",
    "    eval_steps=100,                  # Evaluation step to perform evaluation\n",
    "    save_steps=100,                  # Save checkpoint every X steps\n",
    "    log_level='info',                # Set logging level to info\n",
    "    log_level_replica='info'         # Save checkpoint every X steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=loaded_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "test_results = trainer.evaluate()\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"./grover_model\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"./grover_model\")\n",
    "\n",
    "test_encodings = loaded_tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_dataset = TextDataset(test_encodings, test_labels)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',     # Evaluation strategy to adopt during training\n",
    "    eval_steps=100,                  # Evaluation step to perform evaluation\n",
    "    save_steps=100,                  # Save checkpoint every X steps\n",
    "    log_level='info',                # Set logging level to info\n",
    "    log_level_replica='info'         # Save checkpoint every X steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=loaded_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "test_results = trainer.evaluate()\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = XLMForSequenceClassification.from_pretrained(\"./xlm_model\")\n",
    "loaded_tokenizer = XLMTokenizer.from_pretrained(\"./xlm_model\")\n",
    "\n",
    "test_encodings = loaded_tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_dataset = TextDataset(test_encodings, test_labels)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',     # Evaluation strategy to adopt during training\n",
    "    eval_steps=100,                  # Evaluation step to perform evaluation\n",
    "    save_steps=100,                  # Save checkpoint every X steps\n",
    "    log_level='info',                # Set logging level to info\n",
    "    log_level_replica='info'         # Save checkpoint every X steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=loaded_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "test_results = trainer.evaluate()\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CTRLForSequenceClassification.from_pretrained(\"./ctrl_model\")\n",
    "loaded_tokenizer = CTRLTokenizer.from_pretrained(\"./ctrl_model\")\n",
    "\n",
    "test_encodings = loaded_tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_dataset = TextDataset(test_encodings, test_labels)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',     # Evaluation strategy to adopt during training\n",
    "    eval_steps=100,                  # Evaluation step to perform evaluation\n",
    "    save_steps=100,                  # Save checkpoint every X steps\n",
    "    log_level='info',                # Set logging level to info\n",
    "    log_level_replica='info'         # Save checkpoint every X steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=loaded_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "test_results = trainer.evaluate()\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Models with Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, filepath):\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = self.data['review'].tolist()\n",
    "        # Convert labels to integers\n",
    "        self.labels = torch.tensor(self.data['label'].apply(lambda x: 0 if x == 'human' else 1).tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "        input_ids = inputs['input_ids'].squeeze(0)  # Remove batch dimension\n",
    "        return input_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    true_labels, predictions = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_labels in dataloader:\n",
    "            inputs = {'input_ids': batch_inputs}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_labels = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(predicted_labels.numpy())\n",
    "            true_labels.extend(batch_labels.numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    return accuracy, precision, recall, f1, conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and tokenizer\n",
    "output_dir = './bert_adv_model'\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "test_filepath = 'reviews.csv'\n",
    "test_dataset = TextDataset(tokenizer, test_filepath)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and tokenizer\n",
    "output_dir = './roberta_adv_model'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(output_dir)\n",
    "model = RobertaForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "test_filepath = 'reviews.csv'\n",
    "test_dataset = TextDataset(tokenizer, test_filepath)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and tokenizer\n",
    "output_dir = './gpt2_adv_model'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "test_filepath = 'reviews.csv'\n",
    "test_dataset = TextDataset(tokenizer, test_filepath)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and tokenizer\n",
    "output_dir = './xlnet_adv_model'\n",
    "tokenizer = XLNetTokenizer.from_pretrained(output_dir)\n",
    "model = XLNetForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "test_filepath = 'reviews.csv'\n",
    "test_dataset = TextDataset(tokenizer, test_filepath)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and tokenizer\n",
    "output_dir = './grover_adv_model'\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "test_filepath = 'reviews.csv'\n",
    "test_dataset = TextDataset(tokenizer, test_filepath)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and tokenizer\n",
    "output_dir = './xlm_adv_model'\n",
    "tokenizer = XLMTokenizer.from_pretrained(output_dir)\n",
    "model = XLMForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "test_filepath = 'reviews.csv'\n",
    "test_dataset = TextDataset(tokenizer, test_filepath)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and tokenizer\n",
    "output_dir = './ctrl_adv_model'\n",
    "tokenizer = CTRLTokenizer.from_pretrained(output_dir)\n",
    "model = CTRLForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "test_filepath = 'reviews.csv'\n",
    "test_dataset = TextDataset(tokenizer, test_filepath)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI', 'Human'], yticklabels=['AI', 'Human'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix for Test Set')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
